"""
SERVI√áO DE RAG (Retrieval-Augmented Generation)
--------------------------------------------------
Objetivo:
    Gerenciar a cria√ß√£o, manuten√ß√£o e consulta do "C√©rebro" (Banco Vetorial) da IA.
    √â respons√°vel por transformar arquivos de texto est√°ticos (Markdown) em vetores
    pesquis√°veis (Embeddings) e recuperar as informa√ß√µes mais relevantes para uma pergunta.

Atua√ß√£o no Sistema:
    - Backend / Service: Camada de infraestrutura que abstrai o banco de dados vetorial.
    - Scripting: Pode ser executado via script (`ingest.py`) para popular o banco.

Responsabilidades:
    1. Ler arquivos de documenta√ß√£o (Profile, Projetos) do disco.
    2. Quebrar textos grandes em peda√ßos menores (Chunks).
    3. Gerar vetores num√©ricos usando Google Embeddings.
    4. Gerenciar persist√™ncia no ChromaDB (Vector Store).
    5. Realizar buscas por similaridade sem√¢ntica.

Integra√ß√µes Externas:
    - Google Generative AI (Embeddings): Transforma texto em vetor.
    - ChromaDB: Armazenamento local dos vetores.
"""

import os
import shutil
import time
from typing import List
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from app.core.config import settings

class RagService:
    def __init__(self):
        """
        Inicializa o servi√ßo configurando o modelo de Embeddings e caminhos.
        L√™ as configura√ß√µes globais de `app.core.config`.
        """
        # Inicializa o modelo de Embeddings do Google (gratuito/r√°pido)
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=settings.EMBEDDING_MODEL,
            google_api_key=settings.GOOGLE_API_KEY
        )
        self.persist_directory = os.path.join(os.getcwd(), settings.CHROMA_DB_DIR)
        self.collection_name = settings.COLLECTION_NAME

    def get_vectorstore(self):
        """
        Retorna a conex√£o ativa com o banco vetorial (ChromaDB).
        
        Por que existe: Para permitir que inst√¢ncias do servi√ßo ou callers
        possam realizar opera√ß√µes diretas no banco.
        
        Returns:
            Objeto Chroma configurado e pronto para busca.
        """
        return Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings,
            collection_name=self.collection_name
        )

    def ingest_data(self, data_path: str):
        """
        Executa o pipeline completo de ingest√£o de dados (Indexa√ß√£o).
        
        Fluxo:
        1. Carrega arquivos .md da pasta especificada.
        2. Divide (Split) em chunks menores para caber no contexto da LLM.
        3. Limpa o banco anterior (Full Refresh) para evitar duplicatas.
        4. Insere os novos dados com controle r√≠gido de taxa (Rate Limit) para evitar erro 429 da API do Google.
        
        Args:
            data_path: Caminho absoluto para a pasta contendo os arquivos .md.
        """
        print(f"üìÇ Lendo arquivos de: {data_path}")
        
        if not os.path.exists(data_path):
            print(f"‚ùå Erro: A pasta {data_path} n√£o existe!")
            return

        # --------------------------------------------------
        # 1. Carregamento de Documentos
        # --------------------------------------------------
        # Usa DirectoryLoader com TextLoader for√ßando UTF-8 para evitar erros no Windows.
        loader = DirectoryLoader(
            data_path, 
            glob="**/*.md", 
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"}
        )
        docs = loader.load()
        
        if not docs:
            print("‚ö†Ô∏è Nenhum arquivo encontrado.")
            return

        print(f"üìÑ Encontrados {len(docs)} documentos.")

        # --------------------------------------------------
        # 2. Split (Fragmenta√ß√£o)
        # --------------------------------------------------
        # √â crucial dividir o texto para que a busca retorne apenas o trecho relevante,
        # e n√£o o arquivo inteiro (o que estouraria o token limit).
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # Tamanho alvo de cada peda√ßo
            chunk_overlap=200, # Sobreposi√ß√£o para n√£o perder contexto entre cortes
            separators=["\n# ", "\n## ", "\n### ", "\n", " ", ""] # Tenta cortar em cabe√ßalhos primeiro
        )
        chunks = text_splitter.split_documents(docs)
        print(f"üß© Criados {len(chunks)} chunks de informa√ß√£o.")

        # --------------------------------------------------
        # 3. Limpeza (Full Refresh)
        # --------------------------------------------------
        # Apaga o diret√≥rio f√≠sico do banco para garantir que n√£o haja "lixo" antigo.
        if os.path.exists(self.persist_directory):
            try:
                shutil.rmtree(self.persist_directory)
                print("üßπ Banco antigo limpo.")
            except Exception as e:
                print(f"‚ö†Ô∏è Aviso: N√£o foi poss√≠vel apagar pasta antiga: {e}")

        # --------------------------------------------------
        # 4. Ingest√£o Controlada (Throttling)
        # --------------------------------------------------
        # A API do Google Embeddings tem limites estritos de requisi√ß√µes por minuto (RPM).
        # Implementamos um "Modo Tartaruga" que envia 1 chunk de cada vez com pausas.
        print("üöÄ Iniciando ingest√£o em lotes (Modo Seguro)...")
        
        vectorstore = Chroma(
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory,
            collection_name=self.collection_name
        )

        batch_size = 1   # Envia 1 chunk por request
        delay_seconds = 4 # Espera 4 segundos entre requests
        total_chunks = len(chunks)
        
        for i in range(0, total_chunks, batch_size):
            batch = chunks[i : i + batch_size]
            print(f"   - Processando chunk {i+1} de {total_chunks}...")
            
            try:
                # Tenta adicionar o chunk
                vectorstore.add_documents(documents=batch)
            except Exception as e:
                # Tratamento de erro 429 (Too Many Requests) ou outros erros de rede
                print(f"‚ö†Ô∏è Erro ao processar chunk {i}: {e}")
                print("‚è≥ Erro detectado. Esperando 30s para o Google esfriar a cabe√ßa...")
                time.sleep(30) # Backoff exponencial (manual)
                
                # Tenta novamente (Retry √∫nico)
                try:
                    vectorstore.add_documents(documents=batch)
                    print("   ‚úÖ Recuperado com sucesso.")
                except:
                    print("   ‚ùå Falha definitiva neste chunk. O dado ser√° perdido.")
            
            # Pausa obrigat√≥ria entre iteracoes
            time.sleep(delay_seconds)

        print("‚úÖ Ingest√£o conclu√≠da! Banco salvo.")

    def query(self, question: str, k: int = 4):
        """
        Realiza a busca sem√¢ntica no banco.
        
        Args:
            question: A pergunta ou frase para buscar similaridade.
            k: N√∫mero de resultados para retornar (Top-K).
            
        Returns:
            Lista de Documentos (langchain_core.documents.Document) mais similares.
        """
        vectorstore = self.get_vectorstore() 
        return vectorstore.similarity_search(question, k=k)