"""
M√ìDULO DE GUARDA E FALLBACK (IA / LangGraph)
--------------------------------------------------
Objetivo:
    Centralizar a l√≥gica de decis√£o cr√≠tica sobre "Respondibilidade" (Answerability)
    e gerenciar a comunica√ß√£o de negativas ao usu√°rio (Fallback).
    
Atua√ß√£o no Sistema:
    - Backend / Graph Nodes: Atua como middleware cognitivo entre o n√≥ de Retrieve (RAG)
      e o n√≥ de Gera√ß√£o Final.
      
Responsabilidades:
    1. AnswerabilityGuard: Julgar, via LLM estrito, se o contexto recuperado √© SUFICIENTE
       e SEGURO para responder √† pergunta, evitando alucina√ß√µes e respostas vazias.
    2. FallbackResponder: Gerar a resposta explicativa para o usu√°rio quando o Guard
       bloqueia a gera√ß√£o, mantendo a persona do sistema.

Integra√ß√µes:
    - Recebe dados do n√≥ 'retrieve' e 'contextualize_input'.
    - Comunica-se com o `workflow.py` atrav√©s do estado `agent_app`.
    - Utiliza LLMs (Fast/Medium) para julgamento e gera√ß√£o de texto.
"""

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from app.core.llm import llm_fast, llm_medium, llm_medium_no_temp
from app.graph.state import AgentState
from app.core.logger import logger
import json

# ============================================================================
# N√ì: ANSWERABILITY GUARD
# ============================================================================
def answerability_guard(state: AgentState):
    """
    N√≥ Decis√≥rio (Cognitivo Puro).
    
    Objetivo:
        Avaliar tecnicamente se √© poss√≠vel responder √† pergunta do usu√°rio usando APENAS
        os fatos recuperados (RAG) e o hist√≥rico recente, sem inven√ß√£o.
        Ele atua como um 'Firewall Sem√¢ntico'.

    Regras de Neg√≥cio (Hardening):
        - Fail Closed: Em caso de erro t√©cnico, assume que N√ÉO √© respond√≠vel.
        - Consolida√ß√£o de Contexto: Analisa TODOS os chunks recuperados, n√£o apenas o primeiro.
        - Detec√ß√£o de Exaust√£o: Verifica se o t√≥pico j√° foi esgotado em mensagens anteriores.

    Entrada (State):
        - rephrased_query: A pergunta contextualizada.
        - context: Lista de strings recuperadas do banco vetorial.
        - messages: Hist√≥rico para an√°lise de repeti√ß√£o.

    Sa√≠da (State Update):
        - answerability_result: Dict contendo a decis√£o (is_answerable), motivo e confian√ßa.
    """
    logger.info("--- üõ°Ô∏è ANSWERABILITY GUARD (Julgando viabilidade da resposta...) ---")
    
    messages = state["messages"]
    context = state.get("context", [])
    
    # --------------------------------------------------
    # Prepara√ß√£o de Contexto (Hardening)
    # --------------------------------------------------
    # Une m√∫ltiplos fragmentos de texto retornados pelo Vector DB em uma √∫nica
    # string. Isso garante que o LLM "enxergue" todas as evid√™ncias dispon√≠veis
    # antes de tomar a decis√£o, evitando falsos negativos por fragmenta√ß√£o.
    if isinstance(context, list):
        context_text = "\n\n--- CHUNK SEPARATOR ---\n\n".join(context)
    else:
        context_text = str(context or "")
        
    # Se o RAG n√£o retornou nada, n√£o h√° como responder factualmente.
    # Retorna imediatamente com confian√ßa m√°xima na negativa.
    if not context_text.strip():
        logger.warning("Guard recebeu contexto vazio.")
        return {
            "answerability_result": {
                "is_answerable": False, 
                "reason": "no_context_retrieved", 
                "exhausted": False, 
                "confidence": 1.0
            }
        }
    
    rephrased_query = state.get("rephrased_query") or messages[-1].content
    
    # Simula uma 'mem√≥ria recente' extraindo apenas o que o BOT falou nas √∫ltimas intera√ß√µes.
    # Crucial para evitar que o bot conte a mesma hist√≥ria duas vezes seguida.
    assistant_msgs = [m.content for m in messages[-10:] if m.type == 'assistant']
    previous_answers_summary = "\n---\n".join(assistant_msgs)

    # --------------------------------------------------
    # Defini√ß√£o do Prompt de Julgamento
    # --------------------------------------------------
    # Este prompt for√ßa o LLM a atuar como um classificador l√≥gico,
    # retornando estritamente JSON. Temperatura 0 √© vitual aqui.
    system_prompt = """
    Voc√™ √© o ANSWERABILITY GUARD. Um n√≥ puramente l√≥gico e decis√≥rio.
    Sua fun√ß√£o √© julgar se o contexto recuperado (RAG) √© suficiente e in√©dito para responder √† pergunta do usu√°rio.

    N√ÉO responda √† pergunta.
    N√ÉO seja educado.
    N√ÉO gere texto conversacional.
    APENAS gere um JSON estrito.

    ### INTEN√á√ÉO DO USU√ÅRIO (Contextualizada):
    {query}

    ### CONTEXTO RECUPERADO (Fatos Dispon√≠veis):
    {context}

    ### RESPOSTAS ANTERIORES (O que j√° foi dito):
    {previous_answers}

    ---
    
    ### CRIT√âRIOS DE JULGAMENTO (AND L√≥gico):

    1. **FATO PRESENTE:** Os fatos espec√≠ficos pedidos est√£o EXPLICITAMENTE no contexto?
       - Se pede "ano", tem ano?
       - Se pede "quem", tem nome?
       - Se o contexto √© vago ou n√£o menciona o assunto -> is_answerable = false.

    2. **SEM ALUCINA√á√ÉO:** Voc√™ precisaria inventar algo ou usar conhecimento externo (fora do contexto) para responder satisfatoriamente?
       - Se sim -> is_answerable = false.

    3. **EXAUST√ÉO DE CONTE√öDO (CR√çTICO):**
       - O usu√°rio pediu "mais um", "outro" ou "diferente"?
       - O t√≥pico solicitado j√° foi coberto nas RESPOSTAS ANTERIORES e n√£o h√° NOVOS chunks no contexto sobre isso?
       - Se (TopicRequested == TopicPrevious) AND (NewFacts == Empty/Null) -> is_answerable = false (exhausted = true).

    ### FORMATO DE SA√çDA (JSON OBRIGAT√ìRIO):
    {{
      "is_answerable": boolean,
      "confidence": float, // 0.0 a 1.0 (1.0 = Certeza absoluta, < 0.7 = Incerto)
      "reason": "string_curta_snake_case_descrevendo_o_motivo",
      "exhausted": boolean
    }}

    Exemplos de "reason": 
    - "sufficient_factual_coverage"
    - "missing_specific_fact"
    - "content_exhausted"
    - "ambiguous_intent"
    - "requires_external_knowledge"

    Responda APENAS o JSON.
    """
    
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt)])
    
    # Utiliza modelo sem temperatura (determin√≠stico) para garantir o formato JSON
    # e a estabilidade da decis√£o l√≥gica.
    chain = prompt | llm_medium_no_temp
    
    try:
        response = chain.invoke({
            "query": rephrased_query,
            "context": context_text,
            "previous_answers": previous_answers_summary or "Nenhuma resposta anterior."
        })
        
        content = response.content.strip()
        
        # Higieniza√ß√£o de Markdown: Remove blocos de c√≥digo se o LLM os incluir
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "")
        
        decision_json = json.loads(content)
        
        logger.info(f"Guard Decision: {decision_json}")
        return {"answerability_result": decision_json}
        
    except Exception as e:
        logger.error(f"CRITICAL GUARD FAILURE: {e}")
        
        # --------------------------------------------------
        # Fail Closed Mechanism
        # --------------------------------------------------
        # Se houve erro no processamento cognitivo (ex: JSON malformado, Timeout),
        # assumimos o pior cen√°rio (n√£o responder) para evitar riscos de seguran√ßa/imagem.
        return {
            "answerability_result": {
                "is_answerable": False, 
                "reason": "guard_processing_error", 
                "exhausted": False,
                "confidence": 0.0
            }
        }


# ============================================================================
# N√ì: FALLBACK RESPONDER
# ============================================================================
def fallback_responder(state: AgentState):
    """
    N√≥ de Comunica√ß√£o (Persona).
    
    Objetivo:
        Traduzir a decis√£o t√©cnica do Guard (motivo do bloqueio) em uma explica√ß√£o
        humana, amig√°vel e alinhada √† persona "Marcos".
        
    Por que existe:
        Separa√ß√£o de Preocupa√ß√µes. O n√≥ de gera√ß√£o principal (RAG) n√£o precisa
        aprender a dar desculpas, focando apenas em conte√∫do positivo. Este n√≥
        especializa-se na "arte de dizer n√£o".

    L√≥gica de Resposta Diferenciada:
        Adapta a mensagem baseada no 'reason' recebido (falta de dados, ambiguidade, erro).

    Entrada (State):
        - answerability_result: O veredito do Guard.
    
    Sa√≠da (State Update):
        - messages: Adiciona a resposta final (AIMessage) ao hist√≥rico.
    """
    logger.info("--- üõë FALLBACK RESPONDER (Gerando negativa elegante...) ---")
    
    result = state.get("answerability_result", {})
    reason = result.get("reason", "unknown_reason")
    exhausted = result.get("exhausted", False)
    
    system_prompt = """
    Voc√™ √© o Marcos Rodrigues (Assistant).
    Sua tarefa √© explicar ao usu√°rio que voc√™ N√ÉO consegue responder √† pergunta dele agora.
    
    MOTIVO T√âCNICO: {reason}
    ESGOTAMENTO DE CONTE√öDO: {exhausted}
    
    ### SUAS DIRETRIZES:
    1. **Persona**: Mantenha o tom jovem, dev, direto e 'gente boa'.
       - Use g√≠rias leves se couber ("Putz", "Cara", "Massa").
    
    2. **Se exhausted == true (O usu√°rio pediu mais, mas acabou):**
       - Diga que suas mem√≥rias sobre esse assunto espec√≠fico acabaram.
       - "Cara, sobre [assunto], o que eu tinha de mem√≥ria gravada aqui era isso mesmo."
       - N√£o pe√ßa desculpas profusas. Seja pr√°tico.

    3. **Se missing_fact (N√£o tem a info):**
       - Diga que n√£o tem essa informa√ß√£o no seu "banco de dados" (RAG).
       - Sugira olhar o LinkedIn ou GitHub se fizer sentido.
       - "Putz, essa informa√ß√£o espec√≠fica eu n√£o tenho aqui agora."
       
    4. **Se ambiguous_intent (N√£o entendeu):**
       - Diga que n√£o entendeu se √© sobre X ou Y. Pe√ßa para o usu√°rio reformular.
       - "N√£o tenho certeza se entendi se voc√™ quer saber sobre X ou Y..."

    5. **Se guard_processing_error (Erro Interno):**
       - Diga que teve um solu√ßo t√©cnico r√°pido. Pe√ßa para tentar de novo.
       - "Opa, deu uma travada aqui no meu processamento. Tenta perguntar de novo?"
    
    6. **N√ÉO INVENTE NADA.** O objetivo √© encerrar este t√≥pico com eleg√¢ncia.
    7. **OFERE√áA ALTERNATIVA:** Sugira mudar de assunto ou perguntar sobre outra coisa (Stack, Projetos, Carreira).
    
    Responda diretamente ao usu√°rio.
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt), 
        ("placeholder", "{messages}")
    ])
    
    # Utiliza modelo 'medium' (com temperatura padr√£o) para permitir
    # fluidez e naturalidade na conversa, j√° que n√£o precisamos de output estruturado aqui.
    chain = prompt | llm_medium
    
    response = chain.invoke({
        "messages": state["messages"],
        "reason": reason,
        "exhausted": str(exhausted)
    })
    
    return {"messages": [response]}
